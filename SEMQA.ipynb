{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy import Example\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from dspy.predict.retry import Retry\n",
    "from dspy.datasets import QuoteSum\n",
    "from dsp.utils import EM, normalize_text\n",
    "from dsp.templates.utils import passages2text\n",
    "from dspy.primitives.assertions import assert_transform_module, suggest_backtrack_handler\n",
    "\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo', max_tokens=1000)\n",
    "dspy.settings.configure(lm=turbo, trace=[], temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QuoteSum(path='../../QuoteSum/v1/', train_seed=1, train_size=300, eval_seed=2023, dev_size=300, test_size=0)\n",
    "trainset = [x.with_inputs('question', 'entries') for x in dataset.train]\n",
    "devset = [x.with_inputs('question', 'entries') for x in dataset.dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEMQA formatting helper functions\n",
    "def build_context_string(data_entry):\n",
    "    passages = []\n",
    "    i = 1  \n",
    "    while True:  \n",
    "        title_key = f\"title{i}\"\n",
    "        source_key = f\"source{i}\"\n",
    "        if title_key in data_entry and source_key in data_entry and data_entry[title_key] and data_entry[source_key]:\n",
    "            passage = f\"{data_entry[title_key]}: {data_entry[source_key]}\"\n",
    "            passages.append(passage)\n",
    "        else:\n",
    "            break  \n",
    "        i += 1  \n",
    "    return passages2text(passages)\n",
    "\n",
    "def process_into_passages(text):\n",
    "    passages = re.split(r'\\[\\d+\\]', text)\n",
    "    passages = [passage.strip() for passage in passages if passage.strip()]\n",
    "    modified_passages = []\n",
    "    for i, passage in enumerate(passages, start=1):\n",
    "        match = re.search(r'«.*?:\\s*(.*)»', passage)\n",
    "        if match:\n",
    "            content = match.group(1)\n",
    "            modified_passage = f\"[{i}] {content}\"\n",
    "        else:\n",
    "            modified_passage = f\"[{i}] {passage}\"\n",
    "        modified_passages.append(modified_passage)\n",
    "\n",
    "    return modified_passages\n",
    "\n",
    "def format_quoted_bullets(quoted_bullets_dict):\n",
    "    formatted_bullets = []\n",
    "    for key, bullets in quoted_bullets_dict.items():\n",
    "        for bullet in bullets:\n",
    "            bullet = bullet.lstrip('- ').strip()\n",
    "            formatted_bullet = f\"[ {key} {bullet} ]\"\n",
    "            formatted_bullets.append(formatted_bullet)\n",
    "    return ' '.join(formatted_bullets)\n",
    "\n",
    "def count_unique_sources(data):\n",
    "    count = 0\n",
    "    while True:\n",
    "        title_key = f\"title{count+1}\"\n",
    "        source_key = f\"source{count+1}\"\n",
    "        if title_key in data and source_key in data and data[title_key] and data[source_key]:\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "    return count\n",
    "\n",
    "def contains_marked_tokens(answer):\n",
    "    pattern = r'\\[\\s*\\d+\\s+[^\\]]+\\]'\n",
    "    match = re.search(pattern, answer)\n",
    "    return bool(match)\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "def extract_marked_tokens(answer):\n",
    "    pattern = r'\\[\\s*\\d+\\s*([^\\]]+)\\]'\n",
    "    clean_answer = re.sub(pattern, r'\\1', answer)\n",
    "    return clean_answer\n",
    "\n",
    "def extract_marked_tokens_for_source(answer, source_number):\n",
    "    pattern = r'\\[\\s*{}\\s*([^\\]]+)\\]'.format(source_number)\n",
    "    matches = re.findall(pattern, answer)\n",
    "    return ' '.join(match for match in matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics helper functions\n",
    "def token_f1_score(ref_tokens, gen_tokens):\n",
    "    ref_token_count = Counter(ref_tokens)\n",
    "    gen_token_count = Counter(gen_tokens)\n",
    "    true_positives = ref_token_count & gen_token_count\n",
    "    true_positive_count = sum(true_positives.values())\n",
    "    if true_positive_count == 0:\n",
    "        return 0, 0, 0\n",
    "    precision = true_positive_count / sum(gen_token_count.values())\n",
    "    recall = true_positive_count / sum(ref_token_count.values())\n",
    "    f1 = (2 * precision * recall) / (precision + recall)    \n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def token_recall(ref_tokens, gen_tokens):\n",
    "    ref_token_count = Counter(ref_tokens)\n",
    "    gen_token_count = Counter(gen_tokens)\n",
    "    true_positives = ref_token_count & gen_token_count\n",
    "    true_positive_count = sum(true_positives.values())\n",
    "    recall = true_positive_count / sum(ref_token_count.values()) if ref_token_count else 0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics functions\n",
    "def fluency_metric(references, model_summary):\n",
    "    rouge = Rouge()\n",
    "    max_f_measure = 0\n",
    "    max_precision = 0\n",
    "    max_recall = 0\n",
    "    for i in range(len(references)):\n",
    "        ref_summary = references[i]['summary']\n",
    "        gen_summary = model_summary.quoted_summary\n",
    "        extracted_from_ref = extract_marked_tokens(ref_summary)\n",
    "        extracted_from_gen = extract_marked_tokens(gen_summary)\n",
    "        tokens_ref = tokenize(extracted_from_ref)\n",
    "        tokens_gen = tokenize(extracted_from_gen)\n",
    "        ref_summary_str = ' '.join(tokens_ref)\n",
    "        gen_summary_str = ' '.join(tokens_gen)\n",
    "        scores = rouge.get_scores(gen_summary_str, ref_summary_str)\n",
    "        rouge_l_score = scores[0]['rouge-l']\n",
    "        max_f_measure = max(max_f_measure, rouge_l_score['f'])\n",
    "        max_precision = max(max_precision, rouge_l_score['p'])\n",
    "        max_recall = max(max_recall, rouge_l_score['r'])\n",
    "    return max_f_measure\n",
    "\n",
    "def preciseness(references, model_summary):\n",
    "    gen_summary = model_summary.quoted_summary\n",
    "    source_f1_scores = []\n",
    "    num_sources = count_unique_sources(references[0])\n",
    "    for source_number in range(1, num_sources):\n",
    "        max_f1_for_source = 0\n",
    "        for reference in references:\n",
    "            ref_summary = reference['summary']\n",
    "            extracted_ref_tokens = tokenize(extract_marked_tokens_for_source(ref_summary, source_number))\n",
    "            extracted_gen_tokens = tokenize(extract_marked_tokens_for_source(gen_summary, source_number))\n",
    "            _, _, token_f1 = token_f1_score(extracted_ref_tokens, extracted_gen_tokens)\n",
    "            max_f1_for_source = max(max_f1_for_source, token_f1)\n",
    "        source_f1_scores.append(max_f1_for_source)\n",
    "    average_max_f1_across_sources = np.mean(source_f1_scores)\n",
    "    return average_max_f1_across_sources\n",
    "\n",
    "def comprehensiveness(references, model_summary):\n",
    "    source_recall_scores = []\n",
    "    num_sources = count_unique_sources(references[0])\n",
    "    for source_number in range(1, num_sources + 1):\n",
    "        max_recall_for_source = 0\n",
    "        gen_tokens_for_source = tokenize(extract_marked_tokens_for_source(model_summary.quoted_summary, source_number))\n",
    "        for reference in references:\n",
    "            short_answer_key = f\"short_ans_{source_number}\"\n",
    "            if short_answer_key in reference:\n",
    "                ref_short_answer_tokens = tokenize(reference[short_answer_key])\n",
    "                recall = token_recall(ref_short_answer_tokens, gen_tokens_for_source)\n",
    "                max_recall_for_source = max(max_recall_for_source, recall)\n",
    "        source_recall_scores.append(max_recall_for_source)\n",
    "    average_max_recall_across_sources = np.mean(source_recall_scores)\n",
    "    return average_max_recall_across_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateQuotedText(dspy.Signature):\n",
    "    \"\"\"Extracts bullets of quoted text from passages to answer question.\"\"\"\n",
    "\n",
    "    passage = dspy.InputField(desc=\"structured passage text for analysis\")\n",
    "    question = dspy.InputField()\n",
    "    extracted_quotes = dspy.OutputField(desc=\"bullets of relevant quoted text\")\n",
    "\n",
    "\n",
    "class FormatQuotedSummary(dspy.Signature):\n",
    "    \"\"\"Follow the exact formatting of the Quoted Bullets when using them while creating a cohesive summary paragraph to answer question. \n",
    "    Here is an example:\n",
    "    Quoted Bullets: [ 1 The sky is blue ]. [ 2 The grass is green ].\n",
    "    Question: What are some basic observations about the natural colors in our environment?\n",
    "    Quoted Summary: One source states the [ 1 The sky is blue ] . Another source states [ 2 The grass is green ] .\n",
    "    \"\"\"\n",
    "\n",
    "    # \"\"\"Follow the exact formatting of the Quoted Bullets when using them while creating a cohesive summary paragraph to answer question. \"\"\"\n",
    "\n",
    "    quoted_bullets = dspy.InputField(desc=\"quoted bullets by source number\")\n",
    "    question = dspy.InputField()\n",
    "    quoted_summary = dspy.OutputField(desc=\"quoted summary paragraph that is formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEMQA(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_quoted_text = dspy.ChainOfThought(GenerateQuotedText)\n",
    "        self.generate_formatted_quoted_summary = dspy.ChainOfThought(FormatQuotedSummary)\n",
    "    \n",
    "    def forward(self, question, entries):\n",
    "        references = entries\n",
    "        context_string = build_context_string(references[0])\n",
    "        passages = process_into_passages(context_string)\n",
    "        quoted_bullets_dict = {}\n",
    "        for passage_index in range(len(passages)):\n",
    "            quoted_bullet = self.generate_quoted_text(passage=passages[passage_index], question=question).extracted_quotes\n",
    "            bullet_points = quoted_bullet.split('\\n')\n",
    "            quoted_bullets_dict[passage_index + 1] = bullet_points \n",
    "        formatted_string = format_quoted_bullets(quoted_bullets_dict)\n",
    "        model_summary = self.generate_formatted_quoted_summary(quoted_bullets=formatted_string, question=question)\n",
    "        pred = dspy.Prediction(quoted_summary=model_summary.quoted_summary)\n",
    "        return pred, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEMQA_Assertions(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_quoted_text = dspy.ChainOfThought(GenerateQuotedText)\n",
    "        self.generate_formatted_quoted_summary = dspy.ChainOfThought(FormatQuotedSummary)\n",
    "    \n",
    "    def forward(self, question, entries):\n",
    "        references = entries\n",
    "        context_string = build_context_string(references[0])\n",
    "        passages = process_into_passages(context_string)\n",
    "        quoted_bullets_dict = {}\n",
    "        for passage_index in range(len(passages)):\n",
    "            quoted_bullet = self.generate_quoted_text(passage=passages[passage_index], question=question).extracted_quotes\n",
    "            bullet_points = quoted_bullet.split('\\n')\n",
    "            quoted_bullets_dict[passage_index + 1] = bullet_points \n",
    "        formatted_string = format_quoted_bullets(quoted_bullets_dict)\n",
    "        model_summary = self.generate_formatted_quoted_summary(quoted_bullets=formatted_string, question=question)\n",
    "        pred = dspy.Prediction(quoted_summary=model_summary.quoted_summary)\n",
    "        dspy.Suggest(contains_marked_tokens(pred.quoted_summary), f\"Make the citation formatting is exactly in this format: '[ source_number text ].'.\", target_module=FormatQuotedSummary)\n",
    "        # num_sources = count_unique_sources(references[0])\n",
    "        # for source_number in range(1, num_sources):\n",
    "        #     passage = passages[source_number - 1]\n",
    "        #     extracted_segments = extract_marked_tokens_for_source(pred.quoted_summary, source_number).split('  ')\n",
    "        #     print('extracted_segments')\n",
    "        #     print(extracted_segments)\n",
    "        #     if extracted_segments == ['']:\n",
    "        #         continue\n",
    "\n",
    "        #     all_segments_matched = True\n",
    "        #     for segment in extracted_segments:\n",
    "        #         if segment not in passage:\n",
    "        #             all_segments_matched = False\n",
    "        #             break\n",
    "        #     dspy.Suggest(all_segments_matched, f\"Make sure the quoted summary cites exactly from the passage: '{passage}'.\", target_module=FormatQuotedSummary)\n",
    "        return pred, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(module):\n",
    "    total_fluency_score = 0\n",
    "    total_preciseness_score = 0\n",
    "    total_comprehensiveness_score = 0\n",
    "    total_semqa_score = 0\n",
    "    num_examples = len(devset)\n",
    "    for i in range(len(devset)):\n",
    "        output, references = module(question = devset[i].question, entries = devset[i].entries)\n",
    "        fluency_score = fluency_metric(references, output)\n",
    "        preciseness_score = preciseness(references, output)\n",
    "        comprehensiveness_score = comprehensiveness(references, output)\n",
    "        semqa_score = (preciseness_score * fluency_score) ** 0.5\n",
    "        total_fluency_score += fluency_score\n",
    "        total_preciseness_score += preciseness_score\n",
    "        total_comprehensiveness_score += comprehensiveness_score\n",
    "        total_semqa_score += semqa_score\n",
    "        print(f\"Example {i}: Fluency: {fluency_score}, Preciseness: {preciseness_score}, \"\n",
    "            f\"Comprehensiveness: {comprehensiveness_score}, SEM-QA: {semqa_score}\")\n",
    "\n",
    "    average_fluency_score = total_fluency_score / num_examples\n",
    "    average_preciseness_score = total_preciseness_score / num_examples\n",
    "    average_comprehensiveness_score = total_comprehensiveness_score / num_examples\n",
    "    average_semqa_score = total_semqa_score / num_examples\n",
    "    print(f\"Average Fluency: {average_fluency_score}\")\n",
    "    print(f\"Average Preciseness: {average_preciseness_score}\")\n",
    "    print(f\"Average Comprehensiveness: {average_comprehensiveness_score}\")\n",
    "    print(f\"Average SEM-QA: {average_semqa_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Compilation + No Assertion\n",
    "semqa = SEMQA()\n",
    "evaluate(semqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asserted_SEMQA = assert_transform_module(SEMQA_Assertions().map_named_predictors(Retry), suggest_backtrack_handler)\n",
    "evaluate(asserted_SEMQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
